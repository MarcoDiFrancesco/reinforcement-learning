{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "from sailing import SailingGridworld\n",
    "from common import helper as h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "eps = 1e-4  # TODO: use this value for Task 2 and Task 3\n",
    "value_update_iter = 100  # TODO: change this in Task 2\n",
    "rock_penalty = -2  # TODO: change this in Q1.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "env = SailingGridworld(rock_penalty=rock_penalty, value_update_iter=value_update_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_values_policy(iterations):\n",
    "    v_est = np.zeros((env.w, env.h))\n",
    "    policy = np.zeros((env.w, env.h))\n",
    "    env.draw_values_policy(v_est, policy)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        # TODO: Task 1, implement the value iteration and policy\n",
    "        # TODO: Task 2, convergency of the value function and policy\n",
    "\n",
    "        ########## Your code starts here ##########\n",
    "        # Estimate new state values and calculate policy\n",
    "        v_est_old = v_est.copy()\n",
    "        policy_old = policy.copy()\n",
    "        for x in range(env.w):\n",
    "            for y in range(env.h):\n",
    "                direction_vals = {}\n",
    "                for direction in [env.UP, env.DOWN, env.RIGHT, env.LEFT]:\n",
    "                    # Transitions, e.g.:\n",
    "                    # [\n",
    "                    #      Transition(state=(1, 0), reward=0.0, done=0.0, prob=0.05),\n",
    "                    #      Transition(state=(0, 0), reward=0.0, done=0.0, prob=0.05),\n",
    "                    #      Transition(state=(0, 1), reward=0.0, done=0.0, prob=0.9)\n",
    "                    # ]\n",
    "                    transits = env.transitions[x, y, direction]\n",
    "                    direction_val = 0\n",
    "                    for transit in transits:\n",
    "                        # Set to reward if future state does not exist\n",
    "                        # Transits contains only one transit with probability 1\n",
    "                        if transit.state is None:\n",
    "                            direction_val += 0\n",
    "                        else:\n",
    "                            # x and y transition\n",
    "                            xt, yt = transit.state\n",
    "                            direction_val += transit.prob * (\n",
    "                                transit.reward + (gamma * v_est_old[xt][yt])\n",
    "                            )\n",
    "                    direction_vals[direction] = direction_val\n",
    "\n",
    "                # direction_vals: {0: 7.0, 1: 8.0, 2: 6.0, 3: 6.0}\n",
    "                max_act = max(direction_vals, key=direction_vals.get)  # e.g. 1\n",
    "                max_val = max(direction_vals.values())  # e.g. 8.0\n",
    "                policy[x][y] = max_act\n",
    "                v_est[x][y] = max_val\n",
    "\n",
    "        # Maximum change: should be lower than eps\n",
    "        v_diff = abs(v_est - v_est_old)\n",
    "        v_diff_max = v_diff.max()\n",
    "        # print(\"Value convergence\", v_diff_max, v_diff_max < eps)\n",
    "        # print(\"Policy convergence\", abs(policy_old-policy).max())\n",
    "        print(\"End iteration\", i + 1)\n",
    "        # env.draw_values_policy(v_est, policy)\n",
    "        np.set_printoptions(formatter={\"float\": lambda x: \"{0:0.1f}\".format(x)})\n",
    "        # print(\"POLICY\")\n",
    "        # print(policy)\n",
    "        # print(\"VALUE FUNCTION\")\n",
    "        # print(v_est)\n",
    "        ########## Your code ends here ##########\n",
    "    return v_est, policy\n",
    "\n",
    "\n",
    "# value iteration -- update value estimation and policy\n",
    "value_est, policy = get_values_policy(iterations=value_update_iter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval policy\n",
    "N = 1000  # TODO: change for task 4\n",
    "discounted_rewards = []\n",
    "\n",
    "for ep in range(N):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    step = 0\n",
    "    while not done:\n",
    "\n",
    "        ########## You code starts here ##########\n",
    "\n",
    "        # TODO: Use the policy to take the optimal action (Task 1)\n",
    "        action = policy[state]\n",
    "\n",
    "        # Take a step in the environment\n",
    "        state, reward, done, _ = env.step(action)\n",
    "\n",
    "        # TODO: Calculate discounted return for the initial state\n",
    "        discounted_reward = (gamma**step) * reward\n",
    "        if discounted_reward != 0:\n",
    "            print(ep, discounted_reward)\n",
    "            discounted_rewards.append(discounted_reward)\n",
    "\n",
    "        step += 1\n",
    "        ########## You code ends here ##########\n",
    "    \n",
    "        # Comment out the line below to disable rendering and make computations faster\n",
    "        # env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discounted_rewards\n",
    "print(\"Mean:\", np.mean(discounted_rewards))\n",
    "print(\"Standard deviation:\", np.std(discounted_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save value and policy\n",
    "h.save_object({\"value\": value_est, \"policy\": policy}, \"./value_policy.pkl\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f8497943db1dadd4fbd20ffb51dc189621f1dacb0a290b9761cbed6024b236f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
