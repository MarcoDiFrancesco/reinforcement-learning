{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.insert(0, os.path.abspath(\"..\"))\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\" # for pygame rendering\n",
    "import torch\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from common import helper as h\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load policy from default path to plot\n",
    "policy_dir = Path().cwd() / \"results\" / \"CartPole-v0\" / \"model\"\n",
    "policy_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rbf_agent import RBFAgent\n",
    "\n",
    "rbf = RBFAgent(2)\n",
    "rbf.load(policy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid and initialize arrays to store rewards and actions\n",
    "resolution = 500\n",
    "npoints = resolution\n",
    "x_limit = 4.8\n",
    "theta_limit = 0.5\n",
    "x_range = np.linspace(-x_limit, x_limit, npoints)\n",
    "theta_range = np.linspace(-theta_limit, theta_limit, npoints)\n",
    "# rewards = np.zeros((npoints, npoints))\n",
    "actions = np.zeros((npoints, npoints), dtype=np.int32)\n",
    "\n",
    "for i, th1 in enumerate(x_range):\n",
    "    for j, th2 in enumerate(theta_range):\n",
    "        # Create the state vector from th1, th2\n",
    "        state = np.array([th1, 0, th2, 0])\n",
    "        state = rbf.featurize(state)\n",
    "\n",
    "        action_probs = np.zeros(len(rbf.q_functions)) - np.Inf\n",
    "        for idx, regressor in enumerate(rbf.q_functions):\n",
    "            action_probs[idx] = regressor.predict(state)\n",
    "\n",
    "        actions[i, j] = action_probs.argmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_range.shape, theta_range.shape)\n",
    "actions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the reward plot\n",
    "num_ticks = 10\n",
    "tick_skip = max(1, npoints // num_ticks)\n",
    "# x\n",
    "x_tick_shift = 2 * x_limit / npoints / 2\n",
    "x_tick_points = np.arange(npoints)[::tick_skip] + x_tick_shift\n",
    "x_tick_labels = x_range.round(2)[::tick_skip]\n",
    "# theta\n",
    "theta_tick_shift = 2 * theta_limit / npoints / 2\n",
    "theta_tick_points = np.arange(npoints)[::tick_skip] + theta_tick_shift\n",
    "theta_tick_labels = theta_range.round(2)[::tick_skip]\n",
    "\n",
    "\n",
    "sns.heatmap(actions.T)\n",
    "plt.xticks(theta_tick_points, theta_tick_labels, rotation=45)\n",
    "plt.yticks(x_tick_points, x_tick_labels, rotation=45)\n",
    "plt.xlabel(r\"$\\theta$\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(\"Plot for policy - best action in terms of state\")\n",
    "# plt.suptitle(\"Rewards in %s\" % env_name)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "1f8497943db1dadd4fbd20ffb51dc189621f1dacb0a290b9761cbed6024b236f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
